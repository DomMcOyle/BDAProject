{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fb8d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, explode, monotonically_increasing_id, posexplode\n",
    "from pyspark.sql.functions import udf,struct, collect_list\n",
    "from pyspark.sql.functions import sum as fsum\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import heapq\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99cd6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# cose da implementare:\n",
    "# nota: numerics_domains dovrebbe essere un dataframe con colonne \"idx\"(indice)\n",
    "# \"nome_numerics1\"... e le colonne dei numerics devono essere ordinate \n",
    "#  dataframe con valori unici dei numerici indicizzati (v)\n",
    "#  funzione per calcolare la qualità dei pattern (v)\n",
    "    # modificare il dataset in modo da inserire gli indici al posto dei valori singoli (v)\n",
    "    # modificare funzione di campionamento valori (v)\n",
    "    # aggiungere conteggi globali delle classi (v)\n",
    "#  adattare il tutto sul distribuito\n",
    "    # gestire la memoria meglio\n",
    "# funzione per codificare il dataset\n",
    "#  funzioni per il training + grid search\n",
    "#  funzioni per l'esplorazione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1570180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, target_class, target_col): #t\n",
    "    return data.filter(col(target_col) == target_class)\n",
    "\n",
    "def seq_scout(data, data_plus,target_class, numerics_max, top_k, iterations, theta, alpha): #t\n",
    "    data_support = data.count()\n",
    "    class_support = data_plus.count()\n",
    "    # create priority queue for patterns to be stored\n",
    "    pi = PriorityQueue(k=top_k, theta=theta, cap_length=True) \n",
    "    \n",
    "    # create priority queue for storing each class sequence and its UCB score\n",
    "    scores = PriorityQueue(data_plus)\n",
    "    #N = 1\n",
    "    for N in tqdm(range(1,iterations+1)):\n",
    "        _, Ni, mean_quality, sequence = scores.pop_first() # pop the sequence to be generalized\n",
    "        print\n",
    "        # generalize the sequence and add it to the patterns\n",
    "        gen_seq, new_qual = play_arm(sequence, data, target_class, numerics_max, alpha, data_support, class_support)\n",
    "        pi.add((-new_qual, to_imm_pattern(gen_seq)))\n",
    "        # update the quality and put back the sequence in the priority queue\n",
    "        updated_quality = (Ni * mean_quality + new_qual) / (Ni + 1)\n",
    "        ucb_score = compute_ucb(updated_quality, Ni + 1, N)\n",
    "        scores.add((-ucb_score, Ni + 1, updated_quality, sequence))\n",
    "        \n",
    "        #N += 1\n",
    "    \n",
    "    return pi.get_top_k() # priority queue filters automatically if theta <1\n",
    "\n",
    "def play_arm(sequence, data, target_class, numerics_max, alpha, data_support, class_support): \n",
    "    sequence_m = mutable_seq_copy(sequence)\n",
    "    # get the number of button pressed in the sequence\n",
    "    tot_num_inputs = sum([len(state[0]) for state in sequence])\n",
    "    # get a random number of input to be removed\n",
    "    input_to_remove = random.randint(0, tot_num_inputs-1)\n",
    "\n",
    "    for i in range(input_to_remove):\n",
    "        selected_state_idx = random.randint(0, len(sequence_m)-1)\n",
    "        selected_state = sequence_m[selected_state_idx][0] # we take the input itemset\n",
    "        \n",
    "        selected_state.remove(random.choice(list(selected_state))) # remove an element\n",
    "        \n",
    "        if len(selected_state) == 0: # if the state looses all the inputs, then it is removed\n",
    "            sequence_m.pop(selected_state_idx)\n",
    "    for _, numerics in sequence_m:\n",
    "        for kind, value in numerics.items():\n",
    "            # first we decide whether to remove the constraint or not\n",
    "            if random.random() < alpha:\n",
    "                numerics[kind] = [-float('inf'), float('inf')]\n",
    "            else:              \n",
    "                left_value = random.randint(0, value)\n",
    "                right_value = random.randint(value, numerics_max[kind]-1)\n",
    "\n",
    "                \n",
    "                numerics[kind] = [left_value, right_value]\n",
    "\n",
    "    # now we compute the quality measure\n",
    "    quality = compute_WRAcc(data, sequence_m, target_class, data_support, class_support)\n",
    "\n",
    "    return sequence_m, quality\n",
    "\n",
    "def compute_ucb(score, Ni, N):\n",
    "    # we choose C = 0.5\n",
    "    return (score + 0.25) * 2 + 0.5 * math.sqrt(2 * math.log(N) / Ni)\n",
    "\n",
    "\n",
    "def compute_WRAcc(data, subsequence, target_class, data_support, class_support): \n",
    "    # data support and class support were passed as it is useless to compute them everytime\n",
    "    schema = StructType([\n",
    "        StructField(\"sub_support\", IntegerType(),False),\n",
    "        StructField(\"sub_sup_c\", IntegerType(), False)\n",
    "    ])\n",
    "    udf_subsequence = udf(lambda x,y,z: is_subsequence(subsequence,target_class, x, y, z), schema)\n",
    "    support_data = data.select(udf_subsequence(data.input_sequence,\n",
    "                                               data.enc_num_sequence,\n",
    "                                              col(\"class\")).alias(\"tmp\")).select(fsum(\"tmp.sub_support\").alias(\"sub_support\"),\n",
    "                                                                                 fsum(\"tmp.sub_sup_c\").alias(\"sub_sup_c\"))\n",
    "    sums = support_data.head()\n",
    "    support = sums[\"sub_support\"]\n",
    "    class_pattern_count = sums[\"sub_sup_c\"]\n",
    "\n",
    "    del(sums)\n",
    "    del(support_data)\n",
    "    try:\n",
    "        class_pattern_ratio = class_pattern_count / support\n",
    "    except ZeroDivisionError:\n",
    "        return -0.25\n",
    "\n",
    "    class_data_ratio = class_support / data_support\n",
    "    if support>1:\n",
    "        print(f\"class_pattern_count {class_pattern_count}\")\n",
    "        print(f\"support {support}\")\n",
    "        print(f\"class_pattern_ratio {class_pattern_ratio}\")\n",
    "        print(f\"class_data_ratio {class_data_ratio}\")\n",
    "    wracc = support / data_support * (class_pattern_ratio - class_data_ratio)\n",
    "    return wracc\n",
    "\n",
    "def is_subsequence(subsequence,classsub, sequence_input, sequence_num, classsuper):\n",
    "    # sequence input is a list of lists of strings\n",
    "    # sequence num is a list of rows\n",
    "    i_sub = 0\n",
    "    i_seq = 0\n",
    "    while i_sub<len(subsequence) and i_seq<len(sequence_input):\n",
    "        if subsequence[i_sub][0].issubset(sequence_input[i_seq]):\n",
    "            if all([value >= subsequence[i_sub][1][numeric][0] and value <= subsequence[i_sub][1][numeric][1] for numeric, value in\n",
    "                    sequence_num[i_seq].asDict().items()]):\n",
    "                i_sub += 1\n",
    "        i_seq += 1\n",
    "        \n",
    "    if i_sub == len(subsequence):\n",
    "        is_sub = 1\n",
    "    else:\n",
    "        is_sub = 0\n",
    "    \n",
    "    if classsub is not None:\n",
    "        if is_sub == 1 and classsub == classsuper:\n",
    "            return (is_sub,1)\n",
    "        else:\n",
    "            return (is_sub,0)\n",
    "    else:\n",
    "        return is_sub\n",
    "    \n",
    "#1: function SEQSCOUT(budget)\n",
    "#2: \tπ ← PriorityQueue()\n",
    "#3: \tscores ← PriorityQueue() # ! sfruttare i dataframe distribuiti di spark\n",
    "\n",
    "#8: \t|while budget do \n",
    "#9: \t|\tseq, qual, Ni ← scores.bestUCB()\n",
    "#10: |\tseqp, qualp ← PlayArm(seq) #calcolo qualità parallelizzabile\n",
    "#11: |\tπ.add(seqp, qualp)\n",
    "#12: |\tscores.update(seq,Ni*qual+qualp/Ni+1 , Ni + 1)\n",
    "#3: |end while # while eseguito per ogni top esempio - non parallelizzabile?\n",
    "#4:  \n",
    "#15: return π.topKNonRedundant() # filtering (remove similar starting from the beginning)\n",
    "#16: end function\n",
    "\n",
    "#- il filtering dei dati penso possa essere fatto automaticamente con una bella filter\n",
    "#- controlla come funziona la max del DataFrame\n",
    "#- possibile parallelizzazione 1 per ogni classe (a livello di container -> 7 esecutori max)\n",
    "#- possibile parallelizzazione sul calcolo della metrica come map + reduce\n",
    "#- priority queue con i dataframe distribuiti non ha senso, ma pi può essere implementata easy come una lista\n",
    "#\tche flitra automaticamente i migliori k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eace0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    DISCRETE_INPUTS = {'up', 'accelerate', 'slow', 'goal', 'left', 'boost', 'camera', 'down', 'right', 'slide', 'jump'}\n",
    "    data = []\n",
    "    with open(path, \"r\") as file:\n",
    "        dict_headers = next(file).split()\n",
    "        new_line = dict()\n",
    "        for line in file:\n",
    "            if len(line.split()) <= 1:\n",
    "                if new_line:\n",
    "                    data.append(new_line)\n",
    "                new_line = {\"input_sequence\": [] ,\"num_sequence\":[],\"class\": line.strip()}\n",
    "            else:\n",
    "                if len(dict_headers) != len(line.split()):\n",
    "                    raise ValueError('Number of data and variables do not match')\n",
    "\n",
    "                numerics = {}\n",
    "                buttons = []\n",
    "\n",
    "                for i, value in enumerate(line.split()):\n",
    "                    if dict_headers[i] in DISCRETE_INPUTS:\n",
    "                        if value == '1':\n",
    "                            buttons.append(dict_headers[i])\n",
    "                    else:\n",
    "                        numerics[dict_headers[i]] = float(value)\n",
    "\n",
    "                #state = [buttons, numerics]\n",
    "                new_line[\"input_sequence\"].append(buttons)\n",
    "                new_line[\"num_sequence\"].append(numerics)\n",
    "        data.append(new_line)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1963bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerics(df):\n",
    "    subfields = df.schema[\"num_sequence\"].dataType.elementType.fieldNames()\n",
    "    numerics_domains = {}\n",
    "    numerics_max = {}\n",
    "    for c in subfields:\n",
    "        field = \"num_sequence.\" + c\n",
    "        no_idx = df.select(explode(field).alias(c)).distinct().orderBy(c)\n",
    "        numerics_domains[c] = no_idx.withColumn(\"idx\", monotonically_increasing_id())\n",
    "        numerics_max[\"idx\"+c] = numerics_domains[c].count()\n",
    "    return numerics_domains, numerics_max\n",
    "\n",
    "def convert_numerics(df, numerics_domains):\n",
    "    workdf = df.select(col(\"id\").alias(\"_id\"),posexplode(\"num_sequence\").alias(\"pos\",\"exp\")).select(\"_id\", \"pos\", \"exp.*\")\n",
    "    needed_columns = [i for i in numerics_domains.keys()]\n",
    "    needed_columns.append(\"pos\")\n",
    "    needed_columns.append(\"_id\")\n",
    "    for kind, unique_df in numerics_domains.items():\n",
    "        print(\"processing \" + kind + \"...\")\n",
    "        expr1 = kind + \" as _\" + kind\n",
    "        expr2 = \"idx as idx\" + kind\n",
    "        workdf = workdf.join(unique_df.selectExpr(expr1, expr2), col(kind)==col(\"_\"+kind))\n",
    "        needed_columns.remove(kind)\n",
    "        needed_columns.append(\"idx\"+kind)\n",
    "        workdf = workdf.select(needed_columns)\n",
    "    needed_columns.remove(\"_id\")\n",
    "    needed_columns.remove(\"pos\")\n",
    "    workdf = workdf.orderBy(\"_id\", \"pos\")\n",
    "    workdf = workdf.groupBy(\"_id\", \"pos\").agg(collect_list(struct([col(i) for i in needed_columns])).alias(\"enc_num_sequence\"))\n",
    "    return workdf.groupBy(\"_id\").agg(collect_list(col(\"enc_num_sequence\")[0]).alias(\"enc_num_sequence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "796f6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_imm_sequence(seq):\n",
    "    return tuple([tuple([frozenset(seq[0][i]), tuple(sorted(seq[1][i].asDict().items()))]) for i in range(len(seq[0]))])\n",
    "def mutable_seq_copy(seq):\n",
    "    copy = []\n",
    "    for i in seq:\n",
    "        input_set = set(i[0])\n",
    "        num_dict = {j[0] : j[1] for j in i[1]}\n",
    "        copy.append([input_set, num_dict])\n",
    "    return copy\n",
    "        \n",
    "def to_imm_pattern(pattern):\n",
    "    return tuple([tuple([frozenset(i[0]), tuple(sorted([(key, tuple(value)) for key, value in i[1].items()]))]) for i in\n",
    "                  pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9075253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityQueue(object):\n",
    "    def __init__(self, data=None, k=1,theta=1, cap_length=False):\n",
    "        self.k = k\n",
    "        self.theta=theta\n",
    "        self.cap_length=cap_length if k is not None else False\n",
    "        if data is not None:  \n",
    "            self.heap = [tuple([-float('inf'), 0, 0, import_imm_sequence((x[\"input_sequence\"], x[\"enc_num_sequence\"]))]) for x in data.collect()]\n",
    "            heapq.heapify(self.heap)\n",
    "            if cap_length and len(self.heap)>self.k:\n",
    "                self.heap = heapq.nlargest(self.k, self.heap)\n",
    "            self.seq_set = set([i[-1] for i in self.heap])\n",
    "        else:\n",
    "            self.heap = []\n",
    "            self.seq_set = set()\n",
    "\n",
    "    def add(self, elem):\n",
    "        if elem[-1] not in self.seq_set:\n",
    "            heapq.heappush(self.heap, elem)\n",
    "            self.seq_set.add(elem[-1])\n",
    "            if self.cap_length and len(self.heap)>self.k:\n",
    "                self.heap = heapq.nsmallest(self.k, self.heap)\n",
    "                self.seq_set = set([i[-1] for i in self.heap])\n",
    "                #TODO add filtering if necessary\n",
    "    def pop_first(self):\n",
    "        head = heapq.heappop(self.heap)\n",
    "        self.seq_set.remove(head[-1])\n",
    "        return head\n",
    "    \n",
    "    def get_top_k(self):\n",
    "        if self.theta == 1:\n",
    "            return heapq.nsmallest(self.k, self.heap)\n",
    "        else:\n",
    "            return 0\n",
    "            #TODO add filtering\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f442b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_dataset(\"/vagrant/rocket_league_skillshots.data\")\n",
    "# in case of bigger datasets, single splits could be generated on different nodes\n",
    "# and after joined as single json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316e3982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/07 21:35:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"RocketLeagueFE\").getOrCreate()\n",
    "with open(\"source.json\", \"w\") as s:\n",
    "    s.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13eb44c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"source.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e0184d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- input_sequence: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- num_sequence: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- BallAcceleration: double (nullable = true)\n",
      " |    |    |-- BallSpeed: double (nullable = true)\n",
      " |    |    |-- DistanceBall: double (nullable = true)\n",
      " |    |    |-- DistanceCeil: double (nullable = true)\n",
      " |    |    |-- DistanceWall: double (nullable = true)\n",
      " |    |    |-- PlayerSpeed: double (nullable = true)\n",
      " |    |    |-- Time: double (nullable = true)\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|class|      input_sequence|        num_sequence|\n",
      "+-----+--------------------+--------------------+\n",
      "|    6|[[right, jump], [...|[{1636.7987723122...|\n",
      "|   -1|[[boost, right, j...|[{0.0, 33685.8395...|\n",
      "|   -1|[[right, jump], [...|[{124246.29375405...|\n",
      "|   -1|[[right, slide, j...|[{-8210.634011562...|\n",
      "|   -1|[[right], [boost,...|[{1197.5360615055...|\n",
      "|    6|[[boost, right, j...|[{14578.192522981...|\n",
      "|    1|[[down, right], [...|[{0.0, 170001.715...|\n",
      "|    7|[[right], [right,...|[{4250.8600994742...|\n",
      "|    1|[[right, jump], [...|[{-8323.881952792...|\n",
      "|    6|[[slide], [right]...|[{31754.862957944...|\n",
      "|    2|[[boost, right, j...|[{-301.7738125810...|\n",
      "|    1|[[right], [right,...|[{0.0, 25761.9695...|\n",
      "|    7|[[right, slide], ...|[{0.0, 123286.730...|\n",
      "|    6|[[right, slide, j...|[{6963.6507997466...|\n",
      "|    2|[[right, slide, j...|[{-4735.002465468...|\n",
      "|    1|[[right, slide, j...|[{-9574.096758853...|\n",
      "|    2|[[down, slide], [...|[{-1985.327081700...|\n",
      "|    1|[[right], [boost,...|[{-2166.355471804...|\n",
      "|   -1|[[right], [right,...|[{-5206.415291519...|\n",
      "|    5|[[right], [camera...|[{103772.19254349...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c920c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing BallAcceleration...\n",
      "processing BallSpeed...\n",
      "processing DistanceBall...\n",
      "processing DistanceCeil...\n",
      "processing DistanceWall...\n",
      "processing PlayerSpeed...\n",
      "processing Time...\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- input_sequence: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- enc_num_sequence: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- idxBallAcceleration: long (nullable = false)\n",
      " |    |    |-- idxBallSpeed: long (nullable = false)\n",
      " |    |    |-- idxDistanceBall: long (nullable = false)\n",
      " |    |    |-- idxDistanceCeil: long (nullable = false)\n",
      " |    |    |-- idxDistanceWall: long (nullable = false)\n",
      " |    |    |-- idxPlayerSpeed: long (nullable = false)\n",
      " |    |    |-- idxTime: long (nullable = false)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "numerics_domains, numerics_max = get_numerics(df)\n",
    "encoded_numerics = convert_numerics(df, numerics_domains)\n",
    "dfj = df.join(encoded_numerics, col(\"id\")==col(\"_id\")).select(\"id\",\"input_sequence\" ,\"enc_num_sequence\", \"class\")\n",
    "dfj.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa78c030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: long (nullable = false)\n",
      " |-- enc_num_sequence: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- idxBallAcceleration: long (nullable = false)\n",
      " |    |    |-- idxBallSpeed: long (nullable = false)\n",
      " |    |    |-- idxDistanceBall: long (nullable = false)\n",
      " |    |    |-- idxDistanceCeil: long (nullable = false)\n",
      " |    |    |-- idxDistanceWall: long (nullable = false)\n",
      " |    |    |-- idxPlayerSpeed: long (nullable = false)\n",
      " |    |    |-- idxTime: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_numerics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b93589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idxBallAcceleration': 5747, 'idxBallSpeed': 5958, 'idxDistanceBall': 6762, 'idxDistanceCeil': 3631, 'idxDistanceWall': 5721, 'idxPlayerSpeed': 5942, 'idxTime': 5903}\n"
     ]
    }
   ],
   "source": [
    "print(numerics_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74389bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6375a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▏                                                                           | 64/1000 [02:12<32:48,  2.10s/it]ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "  6%|█████▏                                                                           | 64/1000 [02:13<32:34,  2.09s/it]\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mseq_scout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerics_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[65], line 17\u001b[0m, in \u001b[0;36mseq_scout\u001b[0;34m(data, data_plus, target_class, numerics_max, top_k, iterations, theta, alpha)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# generalize the sequence and add it to the patterns\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m gen_seq, new_qual \u001b[38;5;241m=\u001b[39m \u001b[43mplay_arm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerics_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m pi\u001b[38;5;241m.\u001b[39madd((\u001b[38;5;241m-\u001b[39mnew_qual, to_imm_pattern(gen_seq)))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# update the quality and put back the sequence in the priority queue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[65], line 56\u001b[0m, in \u001b[0;36mplay_arm\u001b[0;34m(sequence, data, target_class, numerics_max, alpha, data_support, class_support)\u001b[0m\n\u001b[1;32m     53\u001b[0m             numerics[kind] \u001b[38;5;241m=\u001b[39m [left_value, right_value]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# now we compute the quality measure\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m quality \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_WRAcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequence_m, quality\n",
      "Cell \u001b[0;32mIn[65], line 76\u001b[0m, in \u001b[0;36mcompute_WRAcc\u001b[0;34m(data, subsequence, target_class, data_support, class_support)\u001b[0m\n\u001b[1;32m     71\u001b[0m udf_subsequence \u001b[38;5;241m=\u001b[39m udf(\u001b[38;5;28;01mlambda\u001b[39;00m x,y,z: is_subsequence(subsequence,target_class, x, y, z), schema)\n\u001b[1;32m     72\u001b[0m support_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mselect(udf_subsequence(data\u001b[38;5;241m.\u001b[39minput_sequence,\n\u001b[1;32m     73\u001b[0m                                            data\u001b[38;5;241m.\u001b[39menc_num_sequence,\n\u001b[1;32m     74\u001b[0m                                           col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mselect(fsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp.sub_support\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_support\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     75\u001b[0m                                                                              fsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp.sub_sup_c\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_sup_c\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 76\u001b[0m sums \u001b[38;5;241m=\u001b[39m \u001b[43msupport_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m support \u001b[38;5;241m=\u001b[39m sums[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_support\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     78\u001b[0m class_pattern_count \u001b[38;5;241m=\u001b[39m sums[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_sup_c\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/sql/dataframe.py:1924\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the first ``n`` rows.\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \n\u001b[1;32m   1899\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1924\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(n)\n",
      "File \u001b[0;32m/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/sql/dataframe.py:1926\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/sql/dataframe.py:868\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;124;03m    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq_scout(dfj, filter_data(dfj, \"1\", \"class\"),\"1\", numerics_max, 30, 1000, 1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd5c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
